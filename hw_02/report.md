N = 1000 | O(n^2): 0.030113с. | O(n):0.000118с. | Ускорение:0.029994с.
N = 10000 | O(n^2): 2.062582с. | O(n):0.000800с. | Ускорение:2.061782с.
N = 100000 | O(n^2): 114.540082с. | O(n):0.008868с. | Ускорение:114.531215с.

Почему O(n²) неприемлемо для N > 10,000?
O(n^2) неприемлемо для N > 10,000 элементов потому что время выполнения кода будет превышать любые рамки приличия. Ожидание по сравнению с O(n), O(1) и O(n^2) будет казаться гигантской пропастью, а это скажется на самом продукте. При покупке билетов на автобус людям придется ждать долгими минутами, если не часами, в сортировке данных тоже возникнут большие проблемы и поиск нужной информации станет просто невыносимым. Именно поэтому нужно обновлять свой код и проектировать его так, чтобы использовать верные алгоритмы для упрощения сложных вещей. Даже любая мелочь может привести к значительной нагрузке на железо, время выполнения и память

Почему O(n) масштабируется линейно?
В худшем случае число шагов будет равно числу элементов, оно ни в коем случае не станет его превышать. К таким можно отнести удаление элемента/его вставку в Массив. При O(n^2) при 3 элементах худший случай будет 9 шагов, при 100 - 10_000 возрастая экспоненциально

Когда выбор структуры данных критически важен?
Выбор структуры данных проекта становится критически важен при масштабировании проекта, в тех случаях когда объем данных становится большим из за чего это может сказаться на работе самого проекта и по мере того, как сам проект будет развиваться. Естественно что некоторые методы которые работали при 10_000 элементах могут не работать при 100_000 и потому будут требовать изменений